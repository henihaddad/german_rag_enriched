{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hanih\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['contexts', 'question', 'answer', 'positive_ctx_idx'],\n",
       "        num_rows: 3362\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets  \n",
    "dataset_name = \"DiscoResearch/germanrag\"  \n",
    "dataset = datasets.load_dataset(dataset_name) \n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-906GDTzXbMY856N7UaJ8tM41rUhPa', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The capital of Germany is Berlin.', role='assistant', function_call=None, tool_calls=None))], created=1709811093, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_2b778c6b35', usage=CompletionUsage(completion_tokens=7, prompt_tokens=24, total_tokens=31))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "#test client connectivity\n",
    "response = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}])\n",
    "response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts, model=\"text-embedding-ada-002\"):\n",
    "    \"\"\"\n",
    "    Fetch embeddings for a list of texts using OpenAI's API.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for text_batch in np.array_split(texts, max(1, len(texts) // 20)):  # Splitting texts into manageable batches\n",
    "        response = client.embeddings.create(input=text_batch.tolist(), model=model)\n",
    "        embeddings += [r.embedding for r in response.data]\n",
    "    return np.array(embeddings)\n",
    "\n",
    "def compute_similarity_scores(embeddings_1, embeddings_2):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity scores between two sets of embeddings.\n",
    "    \"\"\"\n",
    "    # Normalize embeddings to unit length\n",
    "    embeddings_1 = normalize(embeddings_1)\n",
    "    embeddings_2 = normalize(embeddings_2)\n",
    "    \n",
    "    return np.dot(embeddings_1, embeddings_2.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "def generate_hash(*contexts):\n",
    "    \"\"\"Generate a hash for a combination of contexts.\"\"\"\n",
    "    concatenated_contexts = ''.join(sorted(contexts))  # Sort contexts to ensure order doesn't affect hash\n",
    "    return hashlib.md5(concatenated_contexts.encode('utf-8')).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "\n",
    "def filter_candidate_contexts(question_embedding, candidate_contexts_df, pos_context_embedding, hard_negatives_embeddings, similarity_intervals):\n",
    "    \"\"\"\n",
    "    Filters candidate contexts based on similarity scores to the question, positive context, and hard negatives.\n",
    "\n",
    "    Parameters:\n",
    "    - question_embedding: The embedding of the current question.\n",
    "    - candidate_contexts_df: DataFrame of candidate contexts with their embeddings.\n",
    "    - pos_context_embedding: The embedding of the positive context for the current question.\n",
    "    - hard_negatives_embeddings: List of embeddings for hard negative contexts.\n",
    "    - similarity_intervals: A dict containing similarity intervals for question, positive, and hard negatives.\n",
    "\n",
    "    Returns:\n",
    "    - A filtered DataFrame of candidate contexts based on the provided similarity intervals.\n",
    "    \"\"\"\n",
    "    # Calculate similarity scores\n",
    "    candidate_contexts_df= candidate_contexts_df.copy()\n",
    "    similarity_to_question = compute_similarity_scores(question_embedding.reshape(1, -1), candidate_contexts_df['embeddings'].tolist())\n",
    "    similarity_to_question = list(chain.from_iterable(similarity_to_question))\n",
    "    \n",
    "    similarity_to_positive = compute_similarity_scores([pos_context_embedding], candidate_contexts_df['embeddings'].tolist())\n",
    "    similarity_to_positive = list(chain.from_iterable(similarity_to_positive))\n",
    "    \n",
    "    if hard_negatives_embeddings:\n",
    "        similarity_to_negatives = compute_similarity_scores(np.array(hard_negatives_embeddings), np.array(candidate_contexts_df['embeddings'].tolist()))\n",
    "        similarity_to_negatives = np.max(similarity_to_negatives, axis=0)\n",
    "        candidate_contexts_df['similarity_to_negatives'] = similarity_to_negatives\n",
    "    \n",
    "    # Assign similarity scores to DataFrame\n",
    "    candidate_contexts_df['similarity_to_question'] = similarity_to_question\n",
    "    candidate_contexts_df['similarity_to_positive'] = similarity_to_positive\n",
    "\n",
    "    # Filter based on similarity intervals\n",
    "    q_interval = similarity_intervals['question']\n",
    "    p_interval = similarity_intervals['positive']\n",
    "    filtered_df = candidate_contexts_df[(candidate_contexts_df['similarity_to_question'] >= q_interval[0]) & \n",
    "                                        (candidate_contexts_df['similarity_to_question'] <= q_interval[1]) &\n",
    "                                        (candidate_contexts_df['similarity_to_positive'] >= p_interval[0]) & \n",
    "                                        (candidate_contexts_df['similarity_to_positive'] <= p_interval[1])]\n",
    "    \n",
    "    if hard_negatives_embeddings:\n",
    "        n_interval = similarity_intervals['hard_negative']\n",
    "        filtered_df = filtered_df[(filtered_df['similarity_to_negatives'] >= n_interval[0]) & \n",
    "                                  (filtered_df['similarity_to_negatives'] <= n_interval[1])]\n",
    "    \n",
    "    return filtered_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Loaded 250 contexts for 100 questions.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Successfully added easy negatives for question 0.\n",
      "INFO:__main__:Successfully added easy negatives for question 10.\n",
      "INFO:__main__:Successfully added easy negatives for question 20.\n",
      "INFO:__main__:Successfully added easy negatives for question 30.\n",
      "INFO:__main__:Successfully added easy negatives for question 40.\n",
      "INFO:__main__:Successfully added easy negatives for question 50.\n",
      "INFO:__main__:Successfully added easy negatives for question 60.\n",
      "INFO:__main__:Successfully added easy negatives for question 70.\n",
      "INFO:__main__:Successfully added easy negatives for question 80.\n",
      "INFO:__main__:Successfully added easy negatives for question 90.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First question :  Wie viele christlichen Menschen in Deutschland glauben an einen Gott?\n",
      "First easy negative : [\"Römische_Religion\\n\\n==== Aberglaube (superstitio) ====\\nDie Grenze zwischen ''sacra publica'' und ''sacra privata'' dürfte recht durchlässig gewesen sein. Standen die religiösen Praktiken der „einfachen Leute“ aber zu sehr im Widerspruch zur öffentlichen Religionsausübung, wurden sie nicht selten von gelehrter Seite als ''superstitio'' (Aberglaube, Wahnglaube oder übersteigerter Götterglaube) herabgewürdigt, die im Gegensatz zur ''religio'' stünde. Sprachlich gedeutet wurde die ''superstitio'' als ''Überschreiten'' des Staatsglaubens oder als ''Überbleibsel'' eines urtümlich-primitiven Volksglaubens.\\nSeinen pejorativen Sinn erhielt der Begriff mit dem Aufkommen bewusstseinsentrückender Kultformen aus dem hellenistischen Orient, die Bacchanalien als eine der ersten davon wurden am Anfang des 2. Jahrhunderts v. Chr. verboten (Bacchanalienskandal). Als ''superstitio'' galt ein nach altrömischem Denken und Handeln verfehlter Kult, so dass auch das Christentum eine solche war.\", 'Armenien\\n\\n=== Bevölkerungsgruppen ===\\nBevölkerungspyramide Armenien 2016.\\nNach dem letzten Zensus von 2011 waren 98,11 % (2.961.801 Personen) von insgesamt 3.018.854 Einwohnern des Landes ethnische Armenier. Damit kann die Gesamtbevölkerung Armeniens als beinahe ethnisch homogen gelten. Die größte ethnische Minderheit waren mit einem Bevölkerungsanteil von 1,17 % (35.272 Personen) die Jesiden. Dahinter folgten mit 0,39 % (11.911 Personen) Russen, mit 0,09 % (2.769 Personen) Assyrer, mit 0,07 % (2.162 Personen) Kurden, mit 0,04 % (1.176 Personen) Ukrainer, mit 0,03 % (900 Personen) die Gruppe der Pontosgriechen, sowie mit jeweils 0,02 % Georgier (617 Personen) und Iraner (476 Personen). Weiteren verschiedenen Minderheiten gehörten 0,05 % (1.634 Personen) der Bevölkerung an, während 100 Personen eine Aussage zu ihrer Zugehörigkeit im Zensus verweigerten.\\nDamit hatte sich wenig an der ethnischen Zusammensetzung im Vergleich zum Zensus von 2001 verändert. Von den 3.213.011 Einwohnern des Landes bei der 2001 erfolgten Volkszählung waren 3.145.354 Personen oder 97,9 % Armenier, 40.620 oder 1,3 % Jesiden und 14.660 oder 0,5 % Russen (darunter auch Molokanen). Als weitere Minderheiten wurden auch 2001 festgestellt: Kurden (damals mit 1.519 Personen weniger als 2011), Assyrer und Pontosgriechen. Kleinere Gruppen bildeten auch damals Ukrainer, Georgier, sowie ferner Weißrussen, Walachen, Mordwinen, Osseten, Udinen und Taten.\\nDie sehr kleine Minderheit der Kaukasiendeutschen (zwischen 1941–1944 zumeist deportiert) ist heute wie die kleine polnische Minderheit stark russifiziert. Vor dem Bergkarabachkonflikt lebten auch zahlreiche Aseris in Armenien (2,5 % im Jahr 1989).', 'Agnostizismus\\n\\n== Agnostizismus und Theismus ==\\nPrinzipiell sind Agnostizismus und Theismus miteinander vereinbar, denn man kann an einen Gott glauben, auch ohne seine Existenz für gesichert zu halten (Epistemische Logik, z.\\xa0B. Glauben als „Für-Wahrscheinlich-Halten“).\\nIn der Praxis jedoch stehen viele Agnostiker dem Glauben an (konkrete) Gottheiten kritisch gegenüber. Die Gottesbeweise des Theismus (z.\\xa0B. im Judentum, Christentum oder Islam), das Offenbarungswissen und die in Religionen überlieferten Wunder und sonstigen Argumente für die Existenz höherer Wesen halten nach dem Urteil von Agnostikern einer wissenschaftlichen Überprüfung nicht stand. Eine Gottestheorie, die nicht widerlegt werden kann, gilt in den Augen vieler Agnostiker wegen des Fehlens der Falsifizierbarkeit als unwissenschaftlich, wie in der Analogie von „Russells Teekanne“ verdeutlicht wird. Dies sagt zunächst nichts über ihre Wahrheit aus. Sie sollte jedoch nach der als Ockhams Rasiermesser bekannten Denkregel vermieden werden, da sie ein unnötig komplizierter Erklärungsversuch sei. Viele Agnostiker lehnen insbesondere anthropomorphe Gottesvorstellungen ab, da ihnen diese zu stark an die menschliche Kultur und Vorstellungswelt gebunden scheinen.\\nEine Form des Theismus, die von manchen Richtungen des Agnostizismus akzeptiert wird, ist der Pantheismus, der die Welt, die Natur und das Universum als „göttlich“ bezeichnet, ohne darüber hinausgehende Gottheiten zu postulieren. Manche Philosophen, beispielsweise Schopenhauer, bezeichneten den Pantheismus allerdings lediglich als dezenten Atheismus.', \"Theismus\\nTheismus (gr. /ϑεός ''theós'' „Gott“) bezeichnet den Glauben an Götter, wobei der Monotheismus den Glauben an einen Gott und der Polytheismus den Glauben an mehrere Götter bezeichnet.\\nDer Theismus begreift Gott als Schöpfer der Welt, der sie auch erhält und lenkend in sie eingreift. Damit unterscheidet sich der Theismus vom Deismus, der jeden Eingriff eines Gottes in die Welt bestreitet. Der Gott theistischer Religionen ist überwiegend transzendent; teilweise hat er auch immanente Erscheinungsformen oder Elemente. Er wirkt zwar in der Welt (etwa durch Wunder und Offenbarungen), ist jedoch in der Substanz komplett von ihr verschieden (Dualismus von Schöpfer und Schöpfung). Darin unterscheidet der Theismus sich vom Pantheismus und Panentheismus.\\nDie Bezeichnung wurde als ein kategorisierender Begriff der Religionsphilosophie in der Aufklärung (18. Jahrhundert) geprägt gegenüber dem Atheismus, aber auch als Abgrenzung zum Deismus.\", 'Gott\\n\\n==== Klassischer Theismus ====\\nDer Theismus kann zunächst – so etwa bei Richard Swinburne oder John Leslie Mackie – als Gegensatz zum Atheismus, dem Nichtglauben an Götter, betrachtet werden. Hier bezeichnet der Begriff jegliche Weltanschauung, die die Existenz einer göttlichen Instanz annimmt. Im engeren Sinne bezeichnet klassischer Theismus den Glauben an einen oder mehrere Götter, die mit der Welt nicht identisch sind, diese aber lenken und in sie eingreifen, und die eventuell auch ewig und unveränderlich sind.']\n",
      "Second question : Wann werden Kleinspannungsglühlampen aufgrund ihrer Vorteile eingesetzt?\n",
      "Second easy negative : ['Elektromotor\\n\\n== Anwendungen ==\\nElektromotoren kommen sowohl ungeregelt als auch geregelt zum Einsatz. In einfachen Fällen kommen ungeregelte Drehstrommotoren mit Stern-Dreieck-Umschaltungen zur Anwendung. Diese sind jedoch nur zur Lösung primitiver Antriebsaufgaben geeignet. In den meisten Fällen in der heutigen Praxis liegen anspruchsvollere Antriebsprobleme vor, sodass die Elektromotoren durch eine Regelung geregelt werden müssen. Handelt es sich dabei um größere Leistungen, die erforderlich sind, so müssen noch leistungselektronische Stellglieder zwischen Regelung und Elektromotor dazwischengeschaltet werden. Kommen Regelung und Elektromotor zusammen und bilden sie gemeinsam eine funktionelle Einheit, so spricht man vom „Elektroantrieb“. Per se ist also ein Elektromotor nicht an eine Regelung gebunden; in vielen praktischen Fällen hat sich jedoch gerade deren Zusammenwirken als zweckmäßig erwiesen.\\nIn der Vergangenheit fanden Elektromotoren zunächst praktische Verwendung als Antrieb von Straßenbahnen und etwas später als Universalantrieb zur Ersetzung von Dampfmaschinen in Fabriken und wurden zu diesem Zweck über Riementriebe zum Antreiben mechanischer Webstühle und dergleichen eingesetzt. Mit der Einführung von Fließbändern in der Industrie wurden Elektromotoren dann zum Antriebsmittel ganzer Industriezweige schlechthin.\\nIm Bereich Verkehr und Mobilität kamen Elektromotoren erstmals bei Elektrolokomotiven und Elektrischen Bahnen zum Tragen, später in Elektrokarren und in Gabelstaplern. Mit der Weiterentwicklung von Akkus werden heute Elektroautos mit immer größerer Reichweite gebaut und gelten wegen der hohen Effizienz des Elektroantriebs als Alternative zum Verbrennungsmotor in der Zukunft. Entwicklungen in der Leistungselektronik brachten einen weiteren Anwendungsschub – von da ab konnten die wartungsfreien, preiswerten Asynchronmotoren auch für drehzahlvariable Antriebe eingesetzt werden.\\nHeute werden Elektromotoren in großer Zahl in Maschinen, Automaten, Robotern, Spielzeug, Haushaltsgeräten, Elektronikgeräten (zum Beispiel Videorekorder, Festplatten, CD-Spieler), in Ventilatoren, Rasenmähern, Kranen usw. eingesetzt. Die große Bedeutung des Elektromotors für die heutige moderne Industriegesellschaft spiegelt sich auch im Energieverbrauch wider: Elektromotoren haben einen Anteil von über 50 Prozent am Stromverbrauch in Deutschland.']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "from itertools import chain\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_contexts_in_dataframe(dataset, model):\n",
    "    all_contexts = {'context': [], 'question_id':[], 'embeddings':[]}\n",
    "    for i, row in enumerate(dataset['train']):\n",
    "        for context in row['contexts']:\n",
    "            all_contexts['context'].append(context)\n",
    "            all_contexts['question_id'].append(i)\n",
    "\n",
    "    context_embeddings_column = get_embeddings(all_contexts['context'], model=model)\n",
    "    all_contexts['embeddings'] = context_embeddings_column.tolist()\n",
    "    return  pd.DataFrame(all_contexts)\n",
    "\n",
    "\n",
    "def add_easy_negatives(dataset, model=\"text-embedding-ada-002\", question_similarity_interval=(0.5, 0.75), hard_negative_similarity_interval=(0.5, 0.75), positive_similarity_interval=(0.5, 0.75), max_negative_examples=5):\n",
    "    \"\"\"\n",
    "    Add easy negative examples to a dataset by finding similar contexts for each question.\n",
    "    \"\"\"\n",
    "    all_contexts = load_contexts_in_dataframe(dataset,model)\n",
    "    logger.info(f\"Loaded {len(all_contexts)} contexts for {len(dataset['train'])} questions.\")\n",
    "    unique_combinations_set = set()\n",
    "    assert len(all_contexts) == len(all_contexts['context'])\n",
    "    assert all_contexts['question_id'].nunique() == len(dataset['train'])\n",
    "    \n",
    "    all_questions = dataset['train']['question']\n",
    "    question_embeddings = get_embeddings(all_questions, model=model)\n",
    "\n",
    "    easy_negatives=[]\n",
    "    for i, row in enumerate(dataset['train']):\n",
    "        condition = all_contexts['question_id'] == i\n",
    "        current_row_contexts = all_contexts[condition]\n",
    "        candidate_contexts = all_contexts[~condition]\n",
    "        assert len(all_contexts) == len(current_row_contexts) + len(candidate_contexts)\n",
    "\n",
    "        pos_idx=row['positive_ctx_idx']\n",
    "        similarity_intervals = {\n",
    "            'question': question_similarity_interval,\n",
    "            'positive': positive_similarity_interval,\n",
    "            'hard_negative': hard_negative_similarity_interval\n",
    "        }\n",
    "\n",
    "        pos_context_embedding = current_row_contexts.iloc[pos_idx]['embeddings']\n",
    "        hard_negatives_embeddings = [emb for i, emb in enumerate(current_row_contexts['embeddings']) if i != pos_idx]\n",
    "\n",
    "        candidate_contexts = filter_candidate_contexts(question_embeddings[i], candidate_contexts, pos_context_embedding, hard_negatives_embeddings, similarity_intervals)\n",
    "\n",
    "        # checking unicity of the combination\n",
    "        easy_negative_contexts=candidate_contexts['context'].tolist()[:max_negative_examples]\n",
    "        context_combinations_hash = generate_hash(*(easy_negative_contexts + row['contexts']))\n",
    "        while context_combinations_hash in unique_combinations_set and len(easy_negative_contexts) > 0:\n",
    "            easy_negative_contexts.pop()\n",
    "            context_combinations = easy_negative_contexts + row['contexts']\n",
    "            context_combinations_hash = generate_hash(*context_combinations)\n",
    "\n",
    "        easy_negatives.append(easy_negative_contexts)\n",
    "        unique_combinations_set.add(context_combinations_hash)\n",
    "        if(i%10)==0:\n",
    "            logger.info(f\"Successfully added easy negatives for question {i}.\")\n",
    "    dataset['train'] = dataset['train'].add_column(\"easy_negatives\", easy_negatives)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "dataset10 = {'train': dataset['train'].select(range(100))}\n",
    "#we can use text-embedding-3-large for better results default set to text-embedding-ada-002\n",
    "new_dataset = add_easy_negatives(dataset10,model=\"text-embedding-3-large\", question_similarity_interval=(0.3, 0.8), hard_negative_similarity_interval=(0.3, 0.85), positive_similarity_interval=(0.3, 0.8), max_negative_examples=5)['train']\n",
    "print(\"First question : \", new_dataset['question'][0])\n",
    "print(\"First easy negative :\", new_dataset['easy_negatives'][0])\n",
    "print(\"Second question :\", new_dataset['question'][1])\n",
    "print(\"Second easy negative :\", new_dataset['easy_negatives'][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "primary_contexts = []\n",
    "lists_of_hard_negatives = []\n",
    "for i, row in enumerate(new_dataset):\n",
    "    primary_contexts.append(row['contexts'][row['positive_ctx_idx']])\n",
    "    lists_of_hard_negatives.append([row['contexts'][idx] for idx in range(len(row['contexts'])) if idx != row['positive_ctx_idx']])\n",
    "\n",
    "\n",
    "def prepare_dataset_entry(question, context, hard_negatives, easy_negatives, answer):\n",
    "    hard_negatives_formatted = '\\n- '.join(hard_negatives)\n",
    "    easy_negatives_formatted = '\\n- '.join(easy_negatives)\n",
    "    \n",
    "    entry = {\n",
    "        \"context\": f\"Primary Context: {context}\\n\\nSupplementary Contexts:\\nHard Negatives:\\n- {hard_negatives_formatted}\\nEasy Negatives:\\n- {easy_negatives_formatted}\",\n",
    "        \"question\": question,\n",
    "        \"answer\": answer\n",
    "    }\n",
    "    \n",
    "    return entry\n",
    "\n",
    "def export_to_jsonl(entries, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        for entry in entries:\n",
    "            file.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "entries = []\n",
    "for q, c, h_negs, e_negs, a in zip(new_dataset['question'], primary_contexts, lists_of_hard_negatives, new_dataset['easy_negatives'], new_dataset['answer']):\n",
    "    entry = prepare_dataset_entry(q, c, h_negs, e_negs, a)\n",
    "    entries.append(entry)\n",
    "\n",
    "export_to_jsonl(entries, 'enriched_german_rag.jsonl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my approach to enhancing the germanrag dataset for ellamind, I've identified several areas where innovative techniques could significantly improve the dataset's utility for fine-tuning language models on German-language RAG applications. Here are my proposals:\n",
    "\n",
    "1. Dynamic Difficulty Scaling: Recognizing the importance of progressively challenging the model to ensure continuous learning and adaptation, I propose developing an algorithm that dynamically adjusts the difficulty level of questions and associated negatives based on the model's evolving performance. This system would not only ensure that the model is always being pushed to its learning edge but also prevent it from being overwhelmed by too complex questions too soon. Implementing such an algorithm involves categorizing our dataset into tiers of difficulty and incrementally exposing the model to more complex questions as its accuracy and confidence improve.\n",
    "\n",
    "2. Advanced Negative Selection: To further refine the model's ability to discern relevant from irrelevant or misleading information, I suggest an enhancement in our selection of negatives. This involves two key innovations:\n",
    "Algorithmic Refinement: Deploying advanced algorithms that go beyond semantic similarity to include logical and thematic divergence from the question's focus. This could leverage deep learning techniques to assess not just the textual similarity but the contextual relevance and potential for confusion, ensuring the negatives are sophisticated and challenging.\n",
    "Incorporation of Misinformation Negatives: In an era where misinformation is rampant, training the model to identify and disregard such content is crucial. I recommend including negatives that represent common misconceptions or misinformation within the dataset's domain. This strategy will not only improve the model's accuracy but also its applicability in real-world scenarios where discerning truth from falsehood is essential.\n",
    "\n",
    "3. Inclusion of Meta-Data: Understanding the context in which information is presented is pivotal for assessing its relevance and credibility. To this end, I propose augmenting the dataset with meta-data that describes the source, reliability, and date of each context. This addition will enable the model to consider not just the content of the information but also its origin and timeliness, factors that are often critical in determining the accuracy and relevance of an answer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
